{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-393 Máquinas de Aprendizaje II-2019 </h1>\n",
    "\n",
    "<H3 align='center'> Tarea 0 - Introducción a Máquinas de Aprendizaje </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "**Temas**  \n",
    "* Introducción a librerías comunes de *Machine Learning*:\n",
    "    * Pandas\n",
    "    * Numpy\n",
    "    * Sklearn\n",
    "    * Matplotlib\n",
    "    * Otro..\n",
    "* Implementación de Perceptrón y variantes.\n",
    "* Implementación de método aprendizaje online (Gradiente descendente).\n",
    " \n",
    "\n",
    "** Formalidades **  \n",
    "* Equipos de trabajo de: 2 personas\n",
    "* Se debe preparar un (breve) Jupyter/IPython notebook que explique la actividad realizada y las conclusiones del trabajo\n",
    "* Fecha de entrega: 6 de Septiembre.\n",
    "* Formato de entrega: envı́o de link Github al correo electrónico del ayudante (*<francisco.mena.13@sansano.usm.cl>*) , incluyendo al profesor en copia (*<jnancu@inf.utfsm.cl>*). Por favor especificar el siguiente asunto: [Tarea0-INF393-II-2019]\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "\n",
    "La tarea se divide en secciones:\n",
    "\n",
    "[1.](#primero) Perceptrón a mano\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, w, b):\n",
    "    I, J = x.shape\n",
    "    if ((1,J) != w.shape or (1,I) != b.shape):\n",
    "        return -1 * np.ones((I, 1))\n",
    "    return np.where(np.dot(x, w.T) + b.T < 0, 0, 1)\n",
    "\n",
    "def perceptron(x, y, n=0.3):\n",
    "    I, J = x.shape\n",
    "    w = np.zeros((J, 1))\n",
    "    b = np.zeros((I, 1))\n",
    "    mistakes = True\n",
    "    while mistakes:\n",
    "        mistakes = False\n",
    "        for i in range(I):\n",
    "            if y[i] * sign(np.dot(w.T, x[i]) + b[i]) < 0:\n",
    "                w = (w.T + n*(y[i] * x[i])).T\n",
    "                b[i] = b[i] + n*y[i]\n",
    "                mistakes = True\n",
    "    return (w, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"primero\"></a>\n",
    "## 1. Perceptrón a mano\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" width=\"40%\"  />\n",
    "\n",
    "En esta sección se le pedirá que implemente el algoritmo online del *perceptrón* [[2]](#refs) para aprender una función de separación lineal en un problema de clasificación binaria (0 o 1) a través de la función de *treshold*. Un algoritmo online, como el caso del *perceptrón*, aprende de una instancia de dato a la vez $(x^{(i)},y^{(i)})$, dentro de un conjunto de datos $\\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \\ldots, (x^{(N)},y^{(N)})  \\}$, donde la predicción de la clase para cada instancia es través de la función de *treshold*:\n",
    "\n",
    "$$\n",
    "\\hat{y_i} = f(x^{(i)};w,b) = \\left\\{ \\begin{array}{lc}\n",
    "       1 &  si \\ \\sum_j w_j \\cdot x^{(i)}_j +b \\geq \\theta \\\\\n",
    "       0 &  si \\ \\sum_j w_j \\cdot x^{(i)}_j +b < \\theta\n",
    "     \\end{array} \\right.\n",
    "$$\n",
    "\n",
    "\n",
    "Donde $\\theta = 0$. Recordar que el *bias* $b$ se puede incluir dentro de los pesos/parámetros $w$ si se agrega una columna extra de 1's a los datos de entrada $x$ (*como se ve en la imagen anterior*). \n",
    "\n",
    "Para lo que sigue de la actividad sólo podrá utilizar *numpy* (para operaciones de algebra lineal).\n",
    "\n",
    "> a) Escriba una función que calcule el valor de salida (*output*) del modelo $f(x)$ para un patrón de entrada $x$ a través de los pesos $w$ del modelo. *Decida si incluir los bias dentro de $w$ o manejarlos de manera separada*.\n",
    "\n",
    "> b) Escriba una función que implemente el clásico algoritmo del **Perceptrón** para un problema binario que permita entrenarlo en un conjunto de datos de tamaño $N$, leídos de manera *online* (uno a uno). *Recordar la decisión anterior sobre los bias*.\n",
    "\n",
    "> c) Demuestre que lo implementado funciona en un problema real de clasificación. Para esto utilice el dataset **Breast cancer wisconsin**, disponible a través de la librería __[*sklearn*](http://scikit-learn.org)__, el cual corresponde a la detección de cancer mamario a través de características relevantes (numéricas continuas) de un examen realizado, como por ejemplo la textura, simetría y tamaño de una masa mamaria. Estas características deben combinarse linealmente para la detección del cancer.\n",
    "> <div class=\"alert alert-block alert-info\">Es una buena práctica el normalizar los datos antes de trabajar con el modelo</div>\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "X_train,y_train = load_breast_cancer(return_X_y=True)\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_train = np.c_[X_train, np.ones(N) ] #add columns of 1's if you want\n",
    "```\n",
    "\n",
    "Para evaluar los resultados mida la exactitud (*accuracy*) de la clasificación durante el entrenamiento (por cada iteración/instancia/dato) y grafique, utilice el conjunto de entrenamiento realizando una sola pasada (el objetivo de esta sección es familiarizarse con el algoritmo). Además reporte el tiempo de entrenamiento mediante el algoritmo implementado.\n",
    "\n",
    "> d) Escriba una función que implemente el **Forgetrón** [[3]](#refs) con una memoria de tamaño $K$ y la función de kernel como el producto interno (*inner-product*), esto es $<a,b> = \\sum_i a_i \\cdot b_i$.\n",
    "\n",
    "> e) Vuelva a realizar el item c) para el **Forgetrón** con un $K=10$ y compare los resultados.\n",
    "\n",
    "\n",
    "### ¿Qué sucede al variar la función objetivo del problema? \n",
    "Si utilizáramos la función de pérdida *binary cross entropy*, que castiga de manera suave los valores en que se equivoca el modelo a través de que el valor de salida sea una confiabilidad $g(x; w,b) \\in [0,1]$.\n",
    "\n",
    "Realice una modificación al perceptrón para que entregue como salida una confiabilidad continua entre 0 y 1. Una buena aproximación de la función *treshold* (con $\\theta=0$) del perceptrón es la función sigmoidal.\n",
    "\n",
    "<img src=\"https://i.imgur.com/lr6F3Ur.png\" width=\"60%\"  />\n",
    "\n",
    "Ésto sería modelar el perceptrón como:\n",
    "$$\n",
    "g(x^{(i)};w,b) = p(y=1|x^{(i)}) = \\sigma \\left( \\sum_j w_j \\cdot x^{(i)}_j +b \\right)\n",
    "$$\n",
    "\n",
    "Con $\\sigma$ la función sigmoidal de la forma $\\sigma(\\xi) = 1/(1+e^{-\\xi}) $, la cual tiene una derivada cíclica que hace más fácil el cálculo: $\\sigma'(\\xi) = \\sigma(\\xi) (1-\\sigma(\\xi))$\n",
    "\n",
    "> f) Escriba una función que compute la función sigmoidal para una entrada $\\xi$ cualquiera. *Tenga cuidado con los límites de números que puede trabajar python (por ejemplo $\\exp{800}\\rightarrow +\\infty$)*. *Se aconseja acotar/truncar los valores que entran a la función para que la operación se pueda realizar*. Además escriba una función que calcule la salida del nuevo modelo $g(x; w,b)$ con esta función sigmoidal.\n",
    "\n",
    "> g) Escriba una función que calcule la función de pérdida descrita anteriormente para un dato $x^{(i)}$, utilizando $g(x^{(i)};w,b)$. *Tenga cuidado con los límites del logaritmo (recordad que $\\log{0}\\rightarrow +\\infty$)*.\n",
    "\n",
    "> h) Escriba una función que calcule el gradiente (derivada) de la función de pérdida anterior con respecto a los pesos del modelo $w$. *Se recomienda derivarla analíticamente y luego escribirla*. *Recuerde el uso de la regla de la cadena*.\n",
    "\n",
    "> i) Realice una modificación al algoritmo implementado en b) (**Perceptrón**) para que se adapte a la función objetivo *binary cross entropy* implementada, para ésto haga uso del algoritmo de optimización SGD [[4]](#refs) (*Stochastic Gradient Descend*) con tasa de aprendizaje $\\eta \\in [0,1]$.\n",
    "\n",
    "$$ \\vec{w}^{(t+1)} \\leftarrow \\vec{w}^{(t)} - \\eta \\cdot \\nabla_{\\vec{w}^{(t)}} \\ell $$\n",
    "\n",
    "> j) Vuelva a realizar el item c) con esta modificación, además grafique la función de pérdida en el transcurso del entrenamiento. Compare los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
